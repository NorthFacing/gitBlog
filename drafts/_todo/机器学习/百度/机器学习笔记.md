

# 资料

* [深度学习入门](https://github.com/PaddlePaddle/book/blob/develop/README.cn.md)
* [飞桨生态的搜索推荐模型 一 站式开箱即用工具](https://github.com/PaddlePaddle/PaddleRec)

# 学习地址

* [机器学习的思考故事](https://aistudio.baidu.com/aistudio/education/group/info/1138)
* [百度架构师手把手教深度学习](https://aistudio.baidu.com/aistudio/education/group/info/888)
* [哔哩哔哩直播](https://www.bilibili.com/video/BV1zJ411579R?p=1)

# 实践平台
* [AI Studio](https://aistudio.baidu.com/)
* [飞桨官网](https://www.paddlepaddle.org.cn/)
* [飞桨官网 - 零基础实践深度学习](https://www.paddlepaddle.org.cn/tutorials/projectdetail/1211152)
* [百度技术学院](http://bit.baidu.com/index)


# 简要笔记

## 框架拆解

* 假设空间
* 优化目标
* 寻解算法

## 模型

* 阶跃函数：线性回归的输出增加阶跃函数，将线性关系转换为"是、否"两个判断结果：超过阈值得到结果1，否则得到结果0

* 线性回归：优化模型是均方误差
    - 完全拟合已知数据是理想目标，所以优化目标是拟合误差的某种衡量指标，也称为Loss
    - 随着线性回归模型参数w的变化，均方误差Loss随之变化
    - 采用均方误差而不是绝对值：
        - 公式解：极值可进行求导；
        - 梯度下降，容易控制下降的速度：趋势曲线平缓，容易注意到并求得最优点
* 感知机：0/1 误差
    - 二分模型的输出可能有哪些结果：0 良性肿瘤；1 恶性肿瘤
    - 优化目标：min(FN+FP) 最小，也就是不管良性和恶性，判断错误的个数之和最小
    - NP-hard问题：短时间内无法验证（吐血，那这个模型是不能用吗？）
* 逻辑回归：似然误差
    - 概率输出的可能结果：预测输出：0.8（分类概率）；真实分类：0或1（分类标号）
    - 优化目标：最大似然，Sigmoid函数
* 均方误差目标的贝叶斯理解、
    - 如果从合理性考虑，直接使用绝对值误差就可以
    - 如果从易解性考虑，使用均方误差
    
## 寻解算法

* 学习理论
    - 大数定律
    - 过拟合
    - 正则化
    - 校验

* 复杂模型
    - 神经网络 -> 深度学习 -> 表示学习
    - 模型组合 -> 树形模型
    - 空间变换 -> 支撑向量机

* 定值 OR 分布：频率学派 vs 贝叶斯学派
    - 因果律与经典物理：世界是确定的
    - 哥本哈根学派：不确定性与量子物理，比如薛定谔的猫

## 机器能学习的推理过程

* 线性模型的 VC-dim = 特征数量(d) + 1
* 当样本量N >> 特征量d的时候，M<< 2^N
    - M：假设空间，也就是模型复杂度，"所以只要M个假设中任何一个出现乐观欺骗，我就会上当(选中它)"
    - M 较小：简单一些的模型
    - M 过大：复杂一些的模型
* 大数定律的右侧上界：Ω(H,N) = 2Mexp(-2∈^2*N) ≈ 0
    - Ω(H,N) 代表 样本数据和真实数据之间的差距，样本数据是否能够代表真实数据
* 由于大数定律，E(out)-E(in) <= Ω(H,N) >>>> E(in) ≈ E(out)
    - E：error 错误的数量
    - E(in) 样本数据统计错误的数量，读作：in-sample error
    - E(out) 全量的真实数据统计错误的数量，读作：out-sample error
    - 从哪里获取E(out)数据？可以提前预留，不让模型训练使用的数据；也可以重新获取新产生的数据来验证。
* 所以，我们能学到东西(泛化能力)

## 样本量的实际经验：N > 10d(vc)
* 10个特征的线性分类问题，需要100个样本
* 形象案例：选择越多，需要的信息越多 

## 需要权衡的四张面孔
* 第一张
    - 直观感觉
    - 细致 & 置信
* 第二张
    - M过小，E(in) 较大 >>> 欠拟合
    - M过大，E(in) ≠ E(out) >>> 过拟合
    - 欠拟合和过拟合 都不准确，只有找到最合适的模型(最优值)才能误差最小
* 第三张：
    - 实例验证
    - 从欠拟合 到 合适 到 过拟合，效果也从不理想到理想到不理想
* 第四张
    - 深入理解 Bias & Variance
    - 欠拟合 (Bias)：模型能力不足，导致没有捕捉到正确的规律
    - 过拟合 (Variance)：模型能力过强，导致捕捉到很多误差，而不是正确规律
    
## 过拟合的防控
* 更多的样本(增大N)：收集更多样本
* 更准确的资料(减少随机误差 S noise)：数据修正或数据清洗
* 控制模型复杂度(降低复杂度)：

## 如何寻找最优复杂度
* 正则化：寻找可以调和E(in)和Ω(H,N)的目标E(out)
* 校验：故意预留一部分样本，模拟出E(out)的衡量



